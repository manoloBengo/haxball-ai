
Parámetros del modelo:
activation: relu
alpha: 1.0
batch_size: auto
beta_1: 0.9
beta_2: 0.999
early_stopping: False
epsilon: 1e-08
hidden_layer_sizes: (32, 16, 8)
learning_rate: constant
learning_rate_init: 0.001
max_fun: 15000
max_iter: 500
momentum: 0.9
n_iter_no_change: 10
nesterovs_momentum: True
power_t: 0.5
random_state: 42
shuffle: True
solver: adam
tol: 0.0001
validation_fraction: 0.1
verbose: False
warm_start: False

Reporte de clasificación con el mejor modelo:
               precision    recall  f1-score   support

        Blue       0.55      0.48      0.51       276
         Red       0.59      0.46      0.52       276
        none       0.58      0.68      0.62       552

    accuracy                           0.57      1104
   macro avg       0.57      0.54      0.55      1104
weighted avg       0.57      0.57      0.57      1104

