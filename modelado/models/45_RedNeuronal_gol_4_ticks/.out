
Parámetros del modelo:
activation: relu
alpha: 1.0
batch_size: auto
beta_1: 0.9
beta_2: 0.999
early_stopping: False
epsilon: 1e-08
hidden_layer_sizes: (32, 16, 8)
learning_rate: constant
learning_rate_init: 0.001
max_fun: 15000
max_iter: 500
momentum: 0.9
n_iter_no_change: 10
nesterovs_momentum: True
power_t: 0.5
random_state: 42
shuffle: True
solver: adam
tol: 0.0001
validation_fraction: 0.1
verbose: False
warm_start: False

Reporte de clasificación con el mejor modelo:
               precision    recall  f1-score   support

        Blue       0.60      0.71      0.65       217
         Red       0.55      0.34      0.42       216
        none       0.57      0.63      0.60       433

    accuracy                           0.58       866
   macro avg       0.57      0.56      0.56       866
weighted avg       0.57      0.58      0.57       866

